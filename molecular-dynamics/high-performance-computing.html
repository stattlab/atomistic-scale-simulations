
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4.7. HPC (High-Performance Computing) &#8212; Atomistic Scale Simulations</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/image_dark_mode.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/myadmonitions.css?v=5b3d7684" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css?v=80cd6c28" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"d": ["{\\operatorname{d}\\!{#1}}", 1], "dd": ["{\\d{}^{#1} #2 \\over \\d{#3}^{#1}}", 3], "pp": ["{\\partial^{#1} #2 \\over \\partial #3^{#1}}", 3], "td": ["{\\left(\\pp{#1}{#2}{#3} \\right)_{#4}}", 4], "ubar": ["\\underline{#1}", 1], "vv": ["\\mathbf{#1}", 1]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'molecular-dynamics/high-performance-computing';</script>
    <link rel="canonical" href="https://stattlab.github.io/atomistic-scale-simulations/molecular-dynamics/high-performance-computing.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Monte Carlo Simulations" href="../monte-carlo/index.html" />
    <link rel="prev" title="4.6. Initialization and Neighbor Searching" href="neighbor-list.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Atomistic Scale Simulations</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../introduction/index.html">1. Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../introduction/why-atomistic-scale.html">1.1. Why are we interested in the atomistic scale?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction/what-are-simulations.html">1.2. What are Simulations?</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../molecular-models-and-interactions/index.html">2. Molecular Models and Interactions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../molecular-models-and-interactions/introduction.html">2.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../molecular-models-and-interactions/molecular-models.html">2.2. Molecular Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../molecular-models-and-interactions/periodic-boundary-conditions.html">2.3. Periodic Boundary Conditions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../molecular-models-and-interactions/intramolecular-potentials.html">2.4. Intermolecular Potentials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../molecular-models-and-interactions/short-ranged-potentials.html">2.5. Short-Ranged Intramolecular Potentials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../molecular-models-and-interactions/electrostatics.html">2.6. Long-Range Potentials</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../statmech-thermo/index.html">3. Statistical Mechanics and Thermodynamics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../statmech-thermo/introduction.html">3.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statmech-thermo/ensembles.html">3.2. Thermodyanmics and Ensembles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statmech-thermo/statmech-ergodicity.html">3.3. Statistical Mechanics: Ensembles, Ergodicity, and Other Stuff</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statmech-thermo/thermodynamic-properties.html">3.4. Thermodyanmics Properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statmech-thermo/entropy-temperature.html">3.5. Entropy and Temperature</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statmech-thermo/structural-correlations.html">3.6. Structural Correlations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statmech-thermo/transport-coefficients.html">3.7. Transport Coefficients</a></li>
<li class="toctree-l2"><a class="reference internal" href="../statmech-thermo/statistical-errors.html">3.8. Statistical Errors</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">4. Molecular Dynamics</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="introduction.html">4.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="verlet-integration.html">4.2. Verlet Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="thermostats.html">4.3. Thermostats</a></li>
<li class="toctree-l2"><a class="reference internal" href="langevin-dynamics.html">4.4. Langevin Dynamics</a></li>
<li class="toctree-l2"><a class="reference internal" href="md-packages.html">4.5. Using MD Packages</a></li>
<li class="toctree-l2"><a class="reference internal" href="neighbor-list.html">4.6. Initialization and Neighbor Searching</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">4.7. HPC (High-Performance Computing)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../monte-carlo/index.html">5. Monte Carlo Simulations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/introduction.html">5.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/random-numbers.html">5.2. Random Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/monte-carlo-method.html">5.3. Monte Carlo Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/sampling.html">5.4. Sampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/phase-coexistence.html">5.5. Gibbs Ensemble and Phase Coexistence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/finite-size.html">5.6. Phase Transitions, Finite-size Scaling and Renormalization Group</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/variance-reduction.html">5.7. Variance Reduction Techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/metropolis.html">5.8. Metropolis Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/ising-model.html">5.9. The Ising Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/directed-mc.html">5.10. Directed Monte Carlo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/kinetic-mc.html">5.11. Kinetic Monte Carlo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../monte-carlo/free-energy-from-sims.html">5.12. Free Energy Estimation in Simulations</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../references/references.html">6. References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">7. Credits</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/stattlab/atomistic-scale-simulations" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/stattlab/atomistic-scale-simulations/issues/new?title=Issue%20on%20page%20%2Fmolecular-dynamics/high-performance-computing.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/molecular-dynamics/high-performance-computing.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>HPC (High-Performance Computing)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-high-performance-computing-hpc">4.7.1. What is High-Performance Computing (HPC)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#components">4.7.1.1. Components</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-parallelism">4.7.2. Types of Parallelism</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#serial">4.7.2.1. Serial</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#threaded-shared-memory">4.7.2.2. Threaded (Shared Memory)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-distributed-memory">4.7.2.3. Distributed (Distributed Memory)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#communication-patterns">4.7.2.3.1. Communication Patterns</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mpi-x">4.7.2.3.2. MPI + X</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization-in-molecular-dynamics-md">4.7.3. Parallelization in Molecular Dynamics (MD)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#threaded">4.7.3.1. Threaded</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed">4.7.3.2. Distributed</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-performance">4.7.4. Parallel Performance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#strong-scaling">4.7.4.1. Strong Scaling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weak-scaling">4.7.4.2. Weak Scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">4.7.5. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="hpc-high-performance-computing">
<h1><span class="section-number">4.7. </span>HPC (High-Performance Computing)<a class="headerlink" href="#hpc-high-performance-computing" title="Link to this heading">#</a></h1>
<section id="what-is-high-performance-computing-hpc">
<h2><span class="section-number">4.7.1. </span>What is High-Performance Computing (HPC)?<a class="headerlink" href="#what-is-high-performance-computing-hpc" title="Link to this heading">#</a></h2>
<p>High-performance computing (HPC) refers to the use of supercomputers and large compute clusters to solve problems that are too big, too slow, or too data-intensive for a typical desktop or laptop. Instead of running on a single machine with a handful of cores, HPC workloads run across tens, hundreds, or even thousands of processors working together on the same task. The performance of these systems is usually measured in <strong>floating-point operations per second (FLOPS)</strong>. A modern laptop might sustain billions of FLOPS (gigaflops), while even a modest HPC cluster can reach trillions of FLOPS (teraflops), and nationalscale systems operate in the petaflop or exaflop range.</p>
<p>Conceptually, an HPC system is just a large number of ordinary computers connected by a high-speed network and managed as a single machine. These individual computers-called nodes-each have their own processors, memory, and local storage. When you submit a job to the cluster, you are asking the system to reserve some number of these nodes (or CPU cores on them) for a certain amount of time, run your program there, and then return the results. From the user’s point of view, this looks like logging into a login node, writing a job script, and handing that script to a scheduler. Behind the scenes, the cluster’s software stack figures out where and when your job can run.</p>
<p>The key reason to use HPC is that many scientific and engineering problems are naturally parallel. This means that different parts of the computation can be carried out at the same time. For example, you might simulate different spatial regions of a material on different processors, or evaluate forces on different atoms in parallel. To benefit from HPC, your code must be written to exploit this concurrency. A purely serial program-one that uses only one core-will not suddenly become dramatically faster just because it is launched on a supercomputer. At best, it will use a slightly newer CPU. Effective use of HPC therefore requires both appropriate algorithms and explicit parallelization strategies.</p>
<p>Another hallmark of HPC is its emphasis on throughput and scalability rather than user interactivity. Jobs are typically run in batch mode, often for hours or days, and many users share the same cluster. Fair access and efficient utilization are handled by a job scheduler, which queues jobs, allocates resources, and enforces limits. As a result, the workflows and tools used in HPC (batch scripts, modules, environment management, parallel I/O) look very different from those in everyday desktop computing, even though the underlying hardware components are recognizable.</p>
<section id="components">
<h3><span class="section-number">4.7.1.1. </span>Components<a class="headerlink" href="#components" title="Link to this heading">#</a></h3>
<p>Although HPC systems can vary in size and architecture, they share a common set of building blocks. At the heart of the system are the <strong>compute nodes</strong>, the machines that actually execute user jobs. Each node contains one or more <strong>CPUs</strong>, and each CPU provides multiple cores. These <strong>cores</strong> run the main program logic and many parallel threads. In addition to CPUs, many modern nodes also provide accelerators such as GPUs. GPUs contain thousands of simpler cores optimized for massively parallel arithmetic, making them extremely effective for certain workloads that apply the same operations to many independent data elements, have high arithmetic intensity (many floating-point operations per byte moved), and exhibit regular control flow and memory access patterns.</p>
<p>Each node has its own pool of RAM, which serves as local working memory for that node’s processes. The amount of RAM available per node (and per core) places practical limits on the size of problems that can be handled efficiently. If a computation exceeds the memory capacity of a single node, the problem must be distributed across multiple nodes, and the program must explicitly manage the communication of data between them. A common pattern, for example, is to use MPI-based domain decomposition where each process holds only a subregion of a large array (such as a part of a mesh or subset of atoms) and, at each timestep or iteration, exchange ghost or halo data with neighboring processes via message passing so that boundary interactions are correctly updated. Nodes also have <strong>local disk storage</strong> (either hard drives or SSDs), which can be used for temporary files or caching but is usually not the main place where users store data, especially on shared systems.</p>
<p>To support many users and jobs, clusters rely on a <strong>networked file system</strong> that is visible from all nodes. This is typically implemented using a parallel or distributed filesystem such as Lustre, GPFS, or a similar technology. On many systems, users see distinct logical areas. A <strong>home directory</strong> (<code class="docutils literal notranslate"><span class="pre">/home</span></code> ) is usually relatively small, backed up regularly, and intended for scripts, source code, and small configuration files. In contrast, a <strong>scratch directory</strong> (<code class="docutils literal notranslate"><span class="pre">/scratch</span></code> ) is much larger and designed for high-throughput I/O. Scratch storage is fast but not backed up, and files there are often purged periodically. The intended workflow is to run large, I/O-intensive jobs from /scratch , keep only essential results, and move important data back to a more permanent location when the job is finished.</p>
<p>All of these nodes and filesystems are tied together by a high-speed interconnect, the specialized network that lets nodes exchange data with low latency and high bandwidth. This network fabric is critical for distributed parallel applications, where the cost of communication can dominate the overall runtime if the interconnect is slow or congested. High-quality interconnects enable codes to scale to hundreds or thousands of nodes without being completely bottlenecked by communication overhead.</p>
<p>Finally, the entire system is orchestrated by a <strong>scheduler</strong> or resource manager, such as SLURM (used on Campus Cluster here at UIUC), PBS, or similar software. Users do not typically start jobs by directly launching programs on compute nodes. Instead, they write a job script specifying the resources they need, such as the number of nodes, the number of CPU cores per node, the number and type of GPUs, memory per node or per task, maximum runtime, and sometimes additional constraints like node features or queue/partition. A minimal SLURM job script illustrating these fields is shown below. The scheduler places the job in a queue, finds an appropriate set of nodes when they become available, and starts the job with the requested resources. It also enforces policies, such as limits per user or project, and can pack small jobs together on the same node to maximize utilization.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env bash</span>

<span class="c1">#SBATCH -J md_example              # Job name</span>
<span class="c1">#SBATCH -p compute                 # Partition / queue</span>
<span class="c1">#SBATCH -A my_project_account      # Account / project</span>
<span class="c1">#SBATCH -N 2                       # Number of nodes</span>
<span class="c1">#SBATCH --ntasks-per-node=32       # MPI tasks per node</span>
<span class="c1">#SBATCH --gres=gpu:1               # GPUs per node (if needed)</span>
<span class="c1">#SBATCH -t 02:00:00                # Time limit (hh:mm:ss)</span>
<span class="c1">#SBATCH -o md_example_%j.out       # Standard output (%j = job ID)</span>

<span class="c1"># Load any required modules</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">my</span><span class="o">-</span><span class="n">md</span><span class="o">-</span><span class="n">code</span><span class="o">/</span><span class="mf">1.0</span>

<span class="c1"># Run the parallel program</span>
<span class="n">srun</span> <span class="n">my_md_executable</span> <span class="nb">input</span><span class="o">.</span><span class="ow">in</span>
</pre></div>
</div>
</section>
</section>
<section id="types-of-parallelism">
<h2><span class="section-number">4.7.2. </span>Types of Parallelism<a class="headerlink" href="#types-of-parallelism" title="Link to this heading">#</a></h2>
<p>The purpose of parallel computing is to take a computation that would normally run as a single stream of instructions and restructure it so that many operations can be carried out concurrently, allowing the work to be split across multiple hardware resources. In practice, most HPC applications combine several kinds of parallelism. They may exploit vector units inside a core, use multiple threads on a CPU socket, and spread work across many nodes in a cluster.</p>
<p>At a high level, we can distinguish between three main execution models. In <strong>serial execution</strong>, one worker performs all tasks in sequence. In <strong>threaded</strong> or <strong>shared-memory parallelism</strong>, multiple workers (threads) on the same node share a single address space and cooperate through common variables in memory. In <strong>distributed-memory parallelism</strong>, multiple processes, each with its own private memory on possibly different nodes, communicate explicitly by sending messages over the network. Modern large-scale codes often use a hybrid MPI +X model, where MPI handles distributed-memory parallelism across nodes and a secondary model (” <span class="math notranslate nohighlight">\(X\)</span> “) such as OpenMP or CUDA handles shared-memory or accelerator parallelism on each node.</p>
<section id="serial">
<h3><span class="section-number">4.7.2.1. </span>Serial<a class="headerlink" href="#serial" title="Link to this heading">#</a></h3>
<p>In the serial model, the program is executed by a single worker, namely a single operating-system process with a single thread of execution. Instructions are carried out one after another according to the program order, and there is no explicit coordination with other workers because there are no other workers. This model is conceptually simple and is still the starting point for most algorithm development because it is easier to design, debug, and reason about a serial implementation before introducing parallel constructs.</p>
<p>However, serial execution does not scale. Once the algorithm and its implementation are fixed, the only way to make it faster on a single core is to change hardware (e.g., higher clock speed or better microarchitecture) or rely on automatic forms of parallelism such as vectorization. But there is a hard upper bound: the program can never use more than one core at a time. On an HPC cluster with hundreds of thousands of cores available, this means a purely serial code is effectively leaving almost all the machine idle. For problems whose size or desired resolution keeps increasing-larger systems, finer grids, longer trajectories-serial execution quickly becomes a significant bottleneck.</p>
</section>
<section id="threaded-shared-memory">
<h3><span class="section-number">4.7.2.2. </span>Threaded (Shared Memory)<a class="headerlink" href="#threaded-shared-memory" title="Link to this heading">#</a></h3>
<p>Threaded or shared-memory parallelism extends the serial model by allowing multiple workers to run concurrently within the same process, all sharing a common address space. These workers are called <strong>threads</strong>. Each thread executes its own sequence of instructions, but they all see the same global variables and can read or write to the same arrays in memory. On a typical HPC node, threads are mapped to CPU cores. On a GPU, threads are mapped to lightweight hardware execution units.</p>
<p>High-level threading frameworks make it relatively easy to express this pattern. On CPUs, a common approach is to use <strong>OpenMP</strong>, which adds compiler directives (such as <code class="docutils literal notranslate"><span class="pre">#pragma</span> <span class="pre">omp</span> <span class="pre">parallel</span> <span class="pre">for</span></code>) that instruct the compiler to split a loop across multiple threads. On GPUs, <strong>CUDA</strong> or similar frameworks allow kernels to be launched with thousands of threads, each handling one or more elements of the data. In both cases, the programmer identifies parts of the code that can be executed independently-often loops over particles, grid points, or matrix rows-and marks them for parallel execution.</p>
<p>Shared-memory parallelism has two appealing properties. First, it can be adopted incrementally. A working serial code can often be accelerated by adding a relatively small number of directives or annotations to its most time-consuming loops. Second, because all threads share memory, passing data between them is conceptually simple. Any thread can access any variable in the process’s address space without explicit messages. However, this same feature introduces new challenges. When multiple threads update the same data, the final result can depend on the precise timing and interleaving of their operations, leading to subtle and hard-to-reproduce bugs. Avoiding these issues requires synchronization mechanisms such as locks, barriers, and atomic operations. Poorly designed synchronization can easily erase the benefits of parallelism. Moreover, scalability is limited by the resources of a single node, such as the number of cores, the available memory bandwidth, and the cache hierarchy. Once an application has saturated the cores and memory of a node, further speedup requires a move to distributed-memory parallelism.</p>
</section>
<section id="distributed-distributed-memory">
<h3><span class="section-number">4.7.2.3. </span>Distributed (Distributed Memory)<a class="headerlink" href="#distributed-distributed-memory" title="Link to this heading">#</a></h3>
<p>Distributed-memory parallelism is the dominant model for cluster-scale computing. Instead of a single address space shared by all workers, each worker (an operating system process) has its own private memory, often on its own physical node. Processes cannot directly read or write each other’s memory. To exchange data, they must communicate explicitly by sending and receiving messages over the network. The most widely used standard for this model in scientific computing is the <strong>Message Passing Interface (MPI)</strong>.</p>
<p>A typical MPI program begins by launching many processes, often one or several per node. Each process is assigned a unique rank (an integer ID) and runs the same program, but on different portions of the data. For example, in a domain-decomposition scheme, the simulation box is partitioned into spatial subdomains, and each process is responsible for updating the particles or grid points in its subdomain. At each timestep, processes perform local computations, then exchange boundary data with their neighbors so that interactions across subdomain boundaries are correctly accounted for. At the end of a timestep or at output times, processes may send partial results to a designated root process or collectively write to disk.</p>
<p>Because data movement is explicit, distributed-memory programming forces the programmer to think carefully about data layout and communication patterns. The cost of sending messages depends on both latency (how long it takes to initiate a communication) and bandwidth (how many bytes per second can be transferred), both determined by the cluster’s network hardware or fabric. When the computation per process becomes small compared to the amount of data that must be exchanged, communication overhead can dominate the runtime. This is the fundamental reason why scalability is not unlimited: as the number of processes grows, maintaining efficient parallel execution increasingly depends on minimizing and overlapping communication.</p>
<section id="communication-patterns">
<h4><span class="section-number">4.7.2.3.1. </span>Communication Patterns<a class="headerlink" href="#communication-patterns" title="Link to this heading">#</a></h4>
<p>Within MPI and related libraries, communication occurs in a variety of patterns that reflect common algorithmic needs. The simplest is <strong>point-to-point communication</strong>, which occurs when one process sends a message directly to another. This is often used between neighboring subdomains in a spatial decomposition, where each process must exchange halo data (also called ghost data: copies of boundary values from neighboring subdomains needed to compute interactions near the boundary) only with a small set of neighbors.</p>
<p>Many algorithms also require collective operations, where a group of processes cooperates in a higher-level communication pattern. In a <strong>broadcast (point-to-all)</strong>, a single process distributes a piece of data to all others-for example, sending updated parameters or control flags to every process participating in the computation. In a <strong>reduction (all-to-point)</strong>, many processes contribute local values (such as partial sums, minima, or maxima) that are combined and delivered to a single target process. A classic example is computing a global norm or total energy from local contributions. There are also <strong>all-reduce operations</strong>, which combine reduction and broadcast in one step, so that all processes receive the global result, and <strong>all-to-all patterns</strong>, where every process sends potentially different data to every other process.</p>
<p>The performance of these communication patterns depends sensitively on the interconnect. High-bandwidth, low-latency network fabrics-the combination of the interconnect hardware and its topology (how the nodes and switches are connected) -allow collective operations to be implemented efficiently. MPI libraries exploit this by using tree-based or hierarchical algorithms that minimize the number of intermediate links and switches each message must traverse. By shortening these paths and reducing contention on shared links, they help prevent network congestion. On systems with slower or more heavily loaded networks, the same collectives can quickly become bottlenecks. Effective parallel algorithm design therefore involves not only balancing work across processes but also carefully managing how often and how much data is exchanged, and in which pattern.</p>
</section>
<section id="mpi-x">
<h4><span class="section-number">4.7.2.3.2. </span>MPI + X<a class="headerlink" href="#mpi-x" title="Link to this heading">#</a></h4>
<p>As HPC systems have evolved, individual nodes have become more powerful and more complex. A single node may have multiple CPU sockets with many cores, several GPUs, and a deep memory hierarchy. To exploit these resources efficiently, most large-scale applications now use a hybrid parallel model, often referred to as “MPI + X.” In this approach, MPI provides distributed-memory parallelism across nodes (and sometimes across NUMA domains within a node), while “ <span class="math notranslate nohighlight">\(X\)</span> “ represents an additional on-node parallel programming model such as OpenMP threads, CUDA kernels, HIP, SYCL, or another accelerator-focused framework.</p>
<p>In a simple MPI + OpenMP configuration, a cluster might run one MPI process per node, with that process spawning many OpenMP threads to utilize all the cores on the node. The MPI level is responsible for dividing the global problem into large chunks, one per process, and coordinating communication between them. Within each process, OpenMP is used to parallelize loops and kernels over the local data. Similarly, in an MPI + CUDA setup, each MPI process might drive one or more GPUs on its node, offloading compute-intensive kernels while using MPI to exchange data between nodes.</p>
<p>This hybrid strategy offers several advantages. It reduces the total number of MPI processes, which can mitigate pressure on the MPI implementation and the network, particularly in collectives and all-to-all communications. It also aligns naturally with the hardware hierarchy because it mirrors the two main levels of parallel hardware in a modern cluster: MPI handles communication across nodes over the network fabric, while threads or GPU kernels focus on exploiting the high-bandwidth, low-latency resources within each node (shared memory, caches, and device memory). At the same time, it introduces additional layers of complexity in load balancing, memory placement, and synchronization across and within nodes.</p>
<p>Ultimately, the effectiveness of MPI + X parallelism depends heavily on the characteristics of the underlying network hardware. High-bandwidth, low-latency interconnects make it possible for MPI processes to exchange data frequently without stalling the whole computation. If the network is slow or oversubscribed, communication time can overwhelm any gains from adding more nodes, and strong scaling quickly breaks down.</p>
</section>
</section>
</section>
<section id="parallelization-in-molecular-dynamics-md">
<h2><span class="section-number">4.7.3. </span>Parallelization in Molecular Dynamics (MD)<a class="headerlink" href="#parallelization-in-molecular-dynamics-md" title="Link to this heading">#</a></h2>
<p>Molecular dynamics (MD) simulations are a natural fit for HPC because they involve performing very similar computations over and over again on large numbers of particles. At each timestep, the equations of motion are integrated by first computing the forces on every atom, then updating positions and velocities. Among all steps in the algorithm, the force calculation is usually the most expensive. For short-ranged interactions, this means evaluating pairwise forces only for nearby neighbors within a cutoff radius. For long-ranged interactions (e.g., Coulombic forces), additional work is spent on mesh or Ewald-type methods. Either way, the bulk of the runtime is spent in loops that look at many particle pairs and accumulate forces, making them prime targets for parallelization.</p>
<p>Because the same operations are repeated over many atoms or grid points, MD codes can exploit multiple layers of parallelism. At the finest level, vector units and GPU cores handle many pairwise interactions simultaneously. At the next level, multiple CPU cores cooperate via threading on a single node. At the highest level, large simulations are split across many nodes, each responsible for a subregion of the simulation box. Modern production codes typically combine all of these strategies: threaded or GPU kernels accelerate force loops on each node, while distributed-memory parallelism spreads the system across the cluster and coordinates communication between subdomains.</p>
<section id="threaded">
<h3><span class="section-number">4.7.3.1. </span>Threaded<a class="headerlink" href="#threaded" title="Link to this heading">#</a></h3>
<p>In the threaded (shared-memory) approach, an MD simulation running on one node uses multiple CPU cores to evaluate forces and related quantities in parallel. Conceptually, the main loop over atoms or interactions is divided among several threads: each thread is assigned a subset of atoms, neighbor-list entries, or spatial cells, and computes the corresponding contributions to the forces and energies. Because all threads share the same memory, they all see the same arrays of positions, velocities, and forces.</p>
<p>There are different ways to organize this work. One common pattern is to parallelize over atoms: each thread takes a chunk of the atom index range and loops over the neighbors of those atoms. Another pattern parallelizes directly over pair interactions, distributing neighbor-list entries across threads. In both cases, care must be taken when multiple threads update the same force array. Many MD codes avoid these timing-dependent errors by assigning each atom to a “home” thread that accumulates its force, or by using per-thread temporary force buffers that are reduced at the end of the loop.</p>
<p>Threading can deliver substantial speedups on a single node with relatively small changes to a serial code, especially when using OpenMP pragmas or similar directives. However, its scalability is ultimately limited by on-node resources. As the number of threads increases, they compete for the same memory bandwidth and cache hierarchy. At some point, adding more threads produces diminishing returns because the force loop becomes memory-bound rather than compute-bound. Moreover, threading alone cannot extend a simulation beyond the memory capacity of a single node: it can accelerate a given system size but cannot make arbitrarily large systems feasible. For truly large MD simulations, threaded parallelism should be combined with distributed-memory decomposition.</p>
</section>
<section id="distributed">
<h3><span class="section-number">4.7.3.2. </span>Distributed<a class="headerlink" href="#distributed" title="Link to this heading">#</a></h3>
<p>To scale MD beyond a single node, most codes use domain decomposition in a distributed-memory (MPI) setting. The basic idea is to partition the simulation box into spatial subdomains and assign each subdomain to a different MPI rank (process). Each process “owns” the atoms whose positions lie inside its subdomain. It stores their positions, velocities, and forces locally and is responsible for updating them in time. In this way, the global system is broken into many smaller chunks that can be advanced largely in parallel.</p>
<p>Interactions near subdomain boundaries require information about atoms in neighboring regions. To handle this, each process maintains a layer of <strong>ghost particles</strong> (also called halo atoms): copies of atoms that are actually owned by neighboring ranks but lie within the interaction cutoff of the local domain. At each timestep (or whenever needed), neighboring processes exchange boundary data: they send the coordinates of atoms near the boundary to their neighbors, which update their ghost layers. The local process can then compute forces involving both its owned atoms and the ghosts, ensuring that pairwise interactions across subdomain boundaries are correctly included.</p>
<p>As the simulation evolves, atoms move and may cross from one subdomain into another. When this happens, ownership of the atom must be transferred: the old process sends the atom’s state (position, velocity, identity, etc.) to the new process, which adds it to its own list of owned atoms. This particle migration step keeps the domain decomposition consistent with the physical configuration. In practice, domain decomposition and migration are tightly integrated with neighbor-list construction, so that communication and bookkeeping overhead are minimized.</p>
<p>The efficiency of distributed MD parallelization depends on the balance between computation and communication. Each process spends most of its time computing forces for atoms in its subdomain, which roughly scales with the subdomain volume. Communication scales with the surface area of the subdomain, because only atoms near boundaries need to be exchanged. As the number of processes increases for a fixed global system size (strong scaling), subdomains become smaller and more “surface dominated,” and the relative cost of communication grows. Good parallel performance therefore requires both a decomposition that keeps work balanced across ranks and a network fabric with low latency and high bandwidth, so that ghost exchanges and migrations do not dominate the timestep.</p>
</section>
</section>
<section id="parallel-performance">
<h2><span class="section-number">4.7.4. </span>Parallel Performance<a class="headerlink" href="#parallel-performance" title="Link to this heading">#</a></h2>
<p>Even with a powerful cluster and a carefully parallelized code, performance is not unlimited. Some parts of a program are inherently serial, some algorithms do not expose enough independent work, and real hardware introduces overheads from communication, synchronization, memory bandwidth limits, and I/O. Understanding parallel performance means understanding both how much of your code can run concurrently and how efficiently your hardware can keep all workers busy.</p>
<p>A simple but useful model for the limits of parallel speedup is <strong>Amdahl’s Law</strong>. Suppose a fraction <span class="math notranslate nohighlight">\(P\)</span> of your program’s runtime can be perfectly parallelized, while the remaining fraction <span class="math notranslate nohighlight">\(1-P\)</span> is intrinsically serial. If you run on <span class="math notranslate nohighlight">\(N\)</span> processors, the best possible speedup <span class="math notranslate nohighlight">\(S\)</span> (compared to a single processor) is</p>
<div class="math notranslate nohighlight">
\[
S=\frac{1}{(1-P)+\frac{P}{N}}
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(S\)</span> is the ratio of the original serial runtime to the parallel runtime, <span class="math notranslate nohighlight">\(P\)</span> is the parallelizable fraction (between 0 and 1), and <span class="math notranslate nohighlight">\(N\)</span> is the number of processors. As <span class="math notranslate nohighlight">\(N \rightarrow \infty\)</span>, the second term <span class="math notranslate nohighlight">\(P / N\)</span> tends to zero, and the speedup approaches <span class="math notranslate nohighlight">\(1 /(1-P)\)</span>. In other words, no matter how many processors you add, the serial fraction of the code <span class="math notranslate nohighlight">\(1-P\)</span> sets a hard ceiling. If only <span class="math notranslate nohighlight">\(90 \%\)</span> of the code is parallelizable ( <span class="math notranslate nohighlight">\(P=0.9\)</span> ), the maximum speedup is about <span class="math notranslate nohighlight">\(10 \times\)</span>, even on an arbitrarily large machine.</p>
<p>This model is deliberately optimistic because it assumes perfect load balance, no communication costs, and no caches or memory bottlenecks. In real applications, the “effective” serial fraction is often larger because communication, synchronization, and other overheads also scale badly with processor count. Nevertheless, Amdahl’s reminds us that parallel performance is limited not just by hardware, but by algorithmic structure and software design. Improving performance often means refactoring the code to reduce serial regions and to restructure communication patterns, not just throwing more cores at the problem.</p>
<p>To reason more concretely about how your code behaves on a given system, two complementary metrics are commonly used: strong scaling and weak scaling. <strong>Strong scaling</strong> asks, “How much faster can I solve this fixed problem if I increase the number of processors?” <strong>Weak scaling</strong> asks, “How large can I make the problem if I grow it together with the processor count, without blowing up the runtime?”</p>
<section id="strong-scaling">
<h3><span class="section-number">4.7.4.1. </span>Strong Scaling<a class="headerlink" href="#strong-scaling" title="Link to this heading">#</a></h3>
<p>Strong scaling measures how the runtime changes when you solve the same problem on increasing numbers of processors. You fix the total problem size-say, the number of atoms in an MD simulation or the number of grid points in a PDE solve-and then run the code with <span class="math notranslate nohighlight">\(N=1,2,4,8, \ldots\)</span> processors, recording the time to solution in each case. If parallelization were perfect, doubling the number of processors would halve the runtime, and the speedup <span class="math notranslate nohighlight">\(S(N)\)</span> would equal <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>A useful quantity here is the parallel <strong>efficiency</strong>, which compares the achieved speedup to the ideal speedup. If we take the <em>base time</em> to be the runtime on a single processor, then the efficiency at <span class="math notranslate nohighlight">\(N\)</span> processors is</p>
<div class="math notranslate nohighlight">
\[
\text { Efficiency }=\frac{N \times \text { base time }}{\text { actual time }} .
\]</div>
<p>An efficiency of 100% corresponds to linear speedup, which means that every additional processor contributes fully, and the computation scales perfectly.</p>
<p>In practice, efficiency drops as <span class="math notranslate nohighlight">\(N\)</span> increases for several reasons. The serial portion of the code, as captured by Amdahl’s law, becomes more prominent. Once the parallel region has been sped up, even a small serial region can dominate the total runtime. Communication and synchronization overheads also grow with processor count, because collective operations, halo exchanges, and global reductions become more frequent and more expensive relative to the purely local work. At the same time, load imbalance can emerge if the work is not distributed evenly, so that some processors finish early and sit idle while others are still computing. Finally, memory bandwidth and latency can become limiting, especially when many cores on a node compete for access to the same memory channels and caches.</p>
<p>In molecular dynamics, strong scaling is often favorable up to a certain number of cores for a fixed system size, after which the efficiency deteriorates sharply. Beyond that point, the cost of communicating ghost particles and performing global operations such as energy reductions no longer shrinks in proportion to the added compute resources. Strong-scaling studies are therefore essential for deciding how many processors to request for a given simulation. Past some threshold, adding more cores increases the allocation cost or queue time more than it reduces the wall-clock time to solution.</p>
</section>
<section id="weak-scaling">
<h3><span class="section-number">4.7.4.2. </span>Weak Scaling<a class="headerlink" href="#weak-scaling" title="Link to this heading">#</a></h3>
<p>Weak scaling takes a different perspective. Instead of fixing the total problem size, you fix the work per processor and ask how the runtime changes as you increase both the problem size and the number of processors together. For instance, you might assign a fixed number of atoms per core in an MD simulation and then run with <span class="math notranslate nohighlight">\(N=1,2,4,8, \ldots\)</span> cores, each time increasing the total number of atoms proportionally. In an ideal world, the runtime would remain constant so that each processor does the same amount of local work, and any extra overhead from communication is negligible.</p>
<p>In practice, weak scaling reveals how well an algorithm and implementation handle the overheads that grow with system size, which include the communication volume, the cost of collective operations, the pressure on the network fabric, and the increased load on shared services such as filesystems. If the runtime grows slowly with <span class="math notranslate nohighlight">\(N\)</span>, you have good weak scaling meaning that the code can handle larger and larger problems without a disastrous increase in time to solution. If the runtime grows rapidly, then some part of the computation-often communication, I/O, or global synchronization-does not scale with problem size as gracefully as the local work.</p>
<p>From the user’s point of view, weak scaling answers the question, “How big can I go?” If your application shows good weak scaling up to hundreds or thousands of processors, you can use larger allocations to handle much bigger systems than would fit on a single node, while keeping runtimes within reasonable limits. If weak scaling breaks down beyond a certain point, then the machine may still be capable, but the code or algorithm must be redesigned to reduce global communication, restructure data layouts, or exploit different parallel patterns.</p>
</section>
</section>
<section id="references">
<h2><span class="section-number">4.7.5. </span>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p>(1) Department of Computer Science, PBR Visvodaya Institute of Science And Technology, India; Sravanthi, G.; Grace, B.; Kamakshamma, V. A Review of High Performance Computing. <em>IOSRJCE</em> <strong>2014</strong>, <em>16</em> (1), 36–43. <a class="reference external" href="https://doi.org/10.9790/0661-16173643">https://doi.org/10.9790/0661-16173643</a>.</p>
<p>(2) Plimpton, S. “Fast Parallel Algorithms for Short-Range Molecular Dyanmics.” <em>J. Comput. Phys.</em> <strong>1995</strong>, 117, 1-19.</p>
<p>(3) Páll, S.; Zhmurov, A.; Bauer, P.; Abraham, M.; Lundborg, M.; Gray, A.; Hess, B.; Lindahl, E. Heterogeneous Parallelization and Acceleration of Molecular Dynamics Simulations in GROMACS. <em>The Journal of Chemical Physics</em> <strong>2020</strong>, <em>153</em> (13), 134110. <a class="reference external" href="https://doi.org/10.1063/5.0018516">https://doi.org/10.1063/5.0018516</a>.</p>
<p>(4) Hill, M. D.; Marty, M. R. Amdahl’s Law in the Multicore Era.</p>
<p>(5) Lawrence Livermore National Laboratory. <strong>Introduction to Parallel Computing Tutorial;</strong> High Performance Computing.
<a class="reference external" href="https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial?utm_source=chatgpt.com">https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial?utm_source=chatgpt.com</a> (accessed 2025-12-06)</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./molecular-dynamics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="neighbor-list.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">4.6. </span>Initialization and Neighbor Searching</p>
      </div>
    </a>
    <a class="right-next"
       href="../monte-carlo/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Monte Carlo Simulations</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-high-performance-computing-hpc">4.7.1. What is High-Performance Computing (HPC)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#components">4.7.1.1. Components</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-parallelism">4.7.2. Types of Parallelism</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#serial">4.7.2.1. Serial</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#threaded-shared-memory">4.7.2.2. Threaded (Shared Memory)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-distributed-memory">4.7.2.3. Distributed (Distributed Memory)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#communication-patterns">4.7.2.3.1. Communication Patterns</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mpi-x">4.7.2.3.2. MPI + X</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization-in-molecular-dynamics-md">4.7.3. Parallelization in Molecular Dynamics (MD)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#threaded">4.7.3.1. Threaded</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed">4.7.3.2. Distributed</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parallel-performance">4.7.4. Parallel Performance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#strong-scaling">4.7.4.1. Strong Scaling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weak-scaling">4.7.4.2. Weak Scaling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">4.7.5. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Antonia Statt
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>